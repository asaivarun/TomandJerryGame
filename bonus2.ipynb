{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gym_CartPole.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "0_J1BEa33KtG",
        "colab_type": "code",
        "outputId": "7db44691-1bf2-4aef-9b73-3b8cc45c3fca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1091
        }
      },
      "cell_type": "code",
      "source": [
        "!apt install cmake libopenmpi-dev zlib1g-dev \n",
        "#installing dependencies otherwise pip installation will throw error\n",
        "!pip install stable-baselines\n",
        "!apt install python-opengl"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "cmake is already the newest version (3.10.2-1ubuntu2).\n",
            "zlib1g-dev is already the newest version (1:1.2.11.dfsg-0ubuntu2).\n",
            "libopenmpi-dev is already the newest version (2.1.1-8).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 8 not upgraded.\n",
            "Requirement already satisfied: stable-baselines in /usr/local/lib/python3.6/dist-packages (2.2.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from stable-baselines) (4.28.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from stable-baselines) (1.1.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from stable-baselines) (0.13.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from stable-baselines) (2.1.2)\n",
            "Requirement already satisfied: progressbar2 in /usr/local/lib/python3.6/dist-packages (from stable-baselines) (3.38.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from stable-baselines) (1.14.6)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from stable-baselines) (3.4.4.19)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from stable-baselines) (0.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from stable-baselines) (0.22.0)\n",
            "Requirement already satisfied: glob2 in /usr/local/lib/python3.6/dist-packages (from stable-baselines) (0.6)\n",
            "Requirement already satisfied: gym[atari,classic_control]>=0.10.9 in /usr/local/lib/python3.6/dist-packages (from stable-baselines) (0.10.9)\n",
            "Requirement already satisfied: zmq in /usr/local/lib/python3.6/dist-packages (from stable-baselines) (0.0.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from stable-baselines) (0.2.8.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from stable-baselines) (7.0)\n",
            "Requirement already satisfied: mpi4py in /usr/local/lib/python3.6/dist-packages (from stable-baselines) (3.0.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (from stable-baselines) (0.7.1)\n",
            "Requirement already satisfied: tensorflow>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from stable-baselines) (1.12.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines) (1.11.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines) (0.10.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines) (2018.7)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines) (2.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines) (2.5.3)\n",
            "Requirement already satisfied: python-utils>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from progressbar2->stable-baselines) (2.3.0)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]>=0.10.9->stable-baselines) (2.18.4)\n",
            "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]>=0.10.9->stable-baselines) (1.3.2)\n",
            "Requirement already satisfied: Pillow; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]>=0.10.9->stable-baselines) (4.0.0)\n",
            "Requirement already satisfied: atari-py>=0.1.4; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]>=0.10.9->stable-baselines) (0.1.7)\n",
            "Requirement already satisfied: PyOpenGL; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]>=0.10.9->stable-baselines) (3.1.0)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.6/dist-packages (from zmq->stable-baselines) (17.0.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.5.0->stable-baselines) (3.6.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.5.0->stable-baselines) (0.32.3)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.5.0->stable-baselines) (1.0.6)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.5.0->stable-baselines) (1.0.5)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.5.0->stable-baselines) (0.2.0)\n",
            "Requirement already satisfied: tensorboard<1.13.0,>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.5.0->stable-baselines) (1.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.5.0->stable-baselines) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.5.0->stable-baselines) (0.6.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.5.0->stable-baselines) (1.15.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.5.0->stable-baselines) (0.7.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[atari,classic_control]>=0.10.9->stable-baselines) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[atari,classic_control]>=0.10.9->stable-baselines) (2018.11.29)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[atari,classic_control]>=0.10.9->stable-baselines) (1.22)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[atari,classic_control]>=0.10.9->stable-baselines) (2.6)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym[atari,classic_control]>=0.10.9->stable-baselines) (0.16.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow; extra == \"atari\"->gym[atari,classic_control]>=0.10.9->stable-baselines) (0.46)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow>=1.5.0->stable-baselines) (40.6.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow>=1.5.0->stable-baselines) (2.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow>=1.5.0->stable-baselines) (3.0.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow>=1.5.0->stable-baselines) (0.14.1)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python-opengl is already the newest version (3.1.0+dfsg-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 8 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LzIr5OdeLK_Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import gym\n",
        "\n",
        "from stable_baselines.common.vec_env import DummyVecEnv\n",
        "from stable_baselines.deepq.policies import MlpPolicy\n",
        "from stable_baselines import DQN\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from stable_baselines.bench import Monitor\n",
        "from stable_baselines.results_plotter import load_results, ts2xy\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FGwJ8yRiLdQU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "best_mean_reward, n_steps = -np.inf, 0\n",
        "\n",
        "def callback(_locals, _globals):\n",
        "    \"\"\"\n",
        "    Callback called at each step (for DQN an others) or after n steps (see ACER or PPO2)\n",
        "    :param _locals: (dict)\n",
        "    :param _globals: (dict)\n",
        "    \"\"\"\n",
        "    global n_steps, best_mean_reward\n",
        "    # Print stats every 1000 calls\n",
        "    if (n_steps + 1) % 1000 == 0:\n",
        "        # Evaluate policy performance\n",
        "        x, y = ts2xy(load_results(log_dir), 'timesteps')\n",
        "        if len(x) > 0:\n",
        "            mean_reward = np.mean(y[-100:])\n",
        "            print(x[-1], 'timesteps')\n",
        "            print(\"Best mean reward: {:.2f} - Last mean reward per episode: {:.2f}\".format(best_mean_reward, mean_reward))\n",
        "\n",
        "            # New best model, you could save the agent here\n",
        "            if mean_reward > best_mean_reward:\n",
        "                best_mean_reward = mean_reward\n",
        "                # Example for saving best model\n",
        "                print(\"Saving new best model\")\n",
        "                _locals['self'].save(log_dir + 'best_model.pkl')\n",
        "    n_steps += 1\n",
        "    return False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lvjzmwNILLdU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create log dir\n",
        "log_dir = \"/tmp/gym/cartpole/\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lZ9IqRXt3MfO",
        "colab_type": "code",
        "outputId": "3c3c52a6-15e0-4106-e665-6d6343c8f36f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4712
        }
      },
      "cell_type": "code",
      "source": [
        "#Declaring  Environments\n",
        "\n",
        "env = gym.make('CartPole-v1')\n",
        "env = Monitor(env, log_dir, allow_early_resets=True)\n",
        "#env = DummyVecEnv([lambda: env])\n",
        "\n",
        "model = DQN(MlpPolicy, env, verbose=1)\n",
        "model.learn(total_timesteps=100000,callback=callback)\n",
        "#model.save(\"deepq_cartpole\")\n",
        "\n",
        "# del model # remove to demonstrate saving and loading\n",
        "\n",
        "# model = DQN.load(\"deepq_cartpole\")\n",
        "\n",
        "#obs = env.reset()\n",
        "#while True:\n",
        "#    action, _states = model.predict(obs)\n",
        "#    obs, rewards, dones, info = env.step(action)\n",
        "    #env.render()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
            "  result = entry_point.load(False)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "568 timesteps\n",
            "Best mean reward: 91.79 - Last mean reward per episode: 21.04\n",
            "1552 timesteps\n",
            "Best mean reward: 91.79 - Last mean reward per episode: 21.56\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 79       |\n",
            "| episodes                | 100      |\n",
            "| mean 100 episode reward | 21.5     |\n",
            "| steps                   | 2132     |\n",
            "--------------------------------------\n",
            "2521 timesteps\n",
            "Best mean reward: 91.79 - Last mean reward per episode: 21.53\n",
            "3493 timesteps\n",
            "Best mean reward: 91.79 - Last mean reward per episode: 26.27\n",
            "4567 timesteps\n",
            "Best mean reward: 91.79 - Last mean reward per episode: 32.25\n",
            "5503 timesteps\n",
            "Best mean reward: 91.79 - Last mean reward per episode: 39.22\n",
            "6372 timesteps\n",
            "Best mean reward: 91.79 - Last mean reward per episode: 46.61\n",
            "7385 timesteps\n",
            "Best mean reward: 91.79 - Last mean reward per episode: 55.26\n",
            "8439 timesteps\n",
            "Best mean reward: 91.79 - Last mean reward per episode: 64.00\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 11       |\n",
            "| episodes                | 200      |\n",
            "| mean 100 episode reward | 68.8     |\n",
            "| steps                   | 9010     |\n",
            "--------------------------------------\n",
            "9427 timesteps\n",
            "Best mean reward: 91.79 - Last mean reward per episode: 72.10\n",
            "10465 timesteps\n",
            "Best mean reward: 91.79 - Last mean reward per episode: 81.47\n",
            "11571 timesteps\n",
            "Best mean reward: 91.79 - Last mean reward per episode: 90.73\n",
            "12532 timesteps\n",
            "Best mean reward: 91.79 - Last mean reward per episode: 97.72\n",
            "Saving new best model\n",
            "13440 timesteps\n",
            "Best mean reward: 97.72 - Last mean reward per episode: 104.63\n",
            "Saving new best model\n",
            "14465 timesteps\n",
            "Best mean reward: 104.63 - Last mean reward per episode: 111.90\n",
            "Saving new best model\n",
            "15565 timesteps\n",
            "Best mean reward: 111.90 - Last mean reward per episode: 118.44\n",
            "Saving new best model\n",
            "16523 timesteps\n",
            "Best mean reward: 118.44 - Last mean reward per episode: 125.46\n",
            "Saving new best model\n",
            "17504 timesteps\n",
            "Best mean reward: 125.46 - Last mean reward per episode: 133.34\n",
            "Saving new best model\n",
            "18518 timesteps\n",
            "Best mean reward: 133.34 - Last mean reward per episode: 138.48\n",
            "Saving new best model\n",
            "19443 timesteps\n",
            "Best mean reward: 138.48 - Last mean reward per episode: 143.77\n",
            "Saving new best model\n",
            "20496 timesteps\n",
            "Best mean reward: 143.77 - Last mean reward per episode: 146.99\n",
            "Saving new best model\n",
            "21473 timesteps\n",
            "Best mean reward: 146.99 - Last mean reward per episode: 146.97\n",
            "22571 timesteps\n",
            "Best mean reward: 146.99 - Last mean reward per episode: 148.20\n",
            "Saving new best model\n",
            "23490 timesteps\n",
            "Best mean reward: 148.20 - Last mean reward per episode: 150.63\n",
            "Saving new best model\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 300      |\n",
            "| mean 100 episode reward | 152      |\n",
            "| steps                   | 24217    |\n",
            "--------------------------------------\n",
            "24499 timesteps\n",
            "Best mean reward: 150.63 - Last mean reward per episode: 152.07\n",
            "Saving new best model\n",
            "25499 timesteps\n",
            "Best mean reward: 152.07 - Last mean reward per episode: 147.28\n",
            "26512 timesteps\n",
            "Best mean reward: 152.07 - Last mean reward per episode: 151.86\n",
            "27459 timesteps\n",
            "Best mean reward: 152.07 - Last mean reward per episode: 152.91\n",
            "Saving new best model\n",
            "28564 timesteps\n",
            "Best mean reward: 152.91 - Last mean reward per episode: 151.24\n",
            "29459 timesteps\n",
            "Best mean reward: 152.91 - Last mean reward per episode: 146.91\n",
            "30447 timesteps\n",
            "Best mean reward: 152.91 - Last mean reward per episode: 143.47\n",
            "31220 timesteps\n",
            "Best mean reward: 152.91 - Last mean reward per episode: 138.38\n",
            "32490 timesteps\n",
            "Best mean reward: 152.91 - Last mean reward per episode: 144.95\n",
            "33553 timesteps\n",
            "Best mean reward: 152.91 - Last mean reward per episode: 142.89\n",
            "34571 timesteps\n",
            "Best mean reward: 152.91 - Last mean reward per episode: 139.25\n",
            "35467 timesteps\n",
            "Best mean reward: 152.91 - Last mean reward per episode: 138.23\n",
            "36547 timesteps\n",
            "Best mean reward: 152.91 - Last mean reward per episode: 133.70\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 400      |\n",
            "| mean 100 episode reward | 132      |\n",
            "| steps                   | 37373    |\n",
            "--------------------------------------\n",
            "37499 timesteps\n",
            "Best mean reward: 152.91 - Last mean reward per episode: 131.54\n",
            "38523 timesteps\n",
            "Best mean reward: 152.91 - Last mean reward per episode: 131.25\n",
            "39518 timesteps\n",
            "Best mean reward: 152.91 - Last mean reward per episode: 125.68\n",
            "40476 timesteps\n",
            "Best mean reward: 152.91 - Last mean reward per episode: 126.70\n",
            "41565 timesteps\n",
            "Best mean reward: 152.91 - Last mean reward per episode: 128.54\n",
            "42542 timesteps\n",
            "Best mean reward: 152.91 - Last mean reward per episode: 129.62\n",
            "43498 timesteps\n",
            "Best mean reward: 152.91 - Last mean reward per episode: 130.51\n",
            "44477 timesteps\n",
            "Best mean reward: 152.91 - Last mean reward per episode: 128.70\n",
            "45444 timesteps\n",
            "Best mean reward: 152.91 - Last mean reward per episode: 123.93\n",
            "46465 timesteps\n",
            "Best mean reward: 152.91 - Last mean reward per episode: 124.31\n",
            "47565 timesteps\n",
            "Best mean reward: 152.91 - Last mean reward per episode: 126.77\n",
            "48482 timesteps\n",
            "Best mean reward: 152.91 - Last mean reward per episode: 127.81\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 500      |\n",
            "| mean 100 episode reward | 121      |\n",
            "| steps                   | 49450    |\n",
            "--------------------------------------\n",
            "49571 timesteps\n",
            "Best mean reward: 152.91 - Last mean reward per episode: 120.72\n",
            "50482 timesteps\n",
            "Best mean reward: 152.91 - Last mean reward per episode: 121.91\n",
            "51480 timesteps\n",
            "Best mean reward: 152.91 - Last mean reward per episode: 121.95\n",
            "52476 timesteps\n",
            "Best mean reward: 152.91 - Last mean reward per episode: 122.87\n",
            "53485 timesteps\n",
            "Best mean reward: 152.91 - Last mean reward per episode: 123.77\n",
            "54515 timesteps\n",
            "Best mean reward: 152.91 - Last mean reward per episode: 124.79\n",
            "55545 timesteps\n",
            "Best mean reward: 152.91 - Last mean reward per episode: 125.04\n",
            "56521 timesteps\n",
            "Best mean reward: 152.91 - Last mean reward per episode: 122.93\n",
            "57461 timesteps\n",
            "Best mean reward: 152.91 - Last mean reward per episode: 115.05\n",
            "58476 timesteps\n",
            "Best mean reward: 152.91 - Last mean reward per episode: 110.50\n",
            "59461 timesteps\n",
            "Best mean reward: 152.91 - Last mean reward per episode: 111.12\n",
            "60461 timesteps\n",
            "Best mean reward: 152.91 - Last mean reward per episode: 113.33\n",
            "61535 timesteps\n",
            "Best mean reward: 152.91 - Last mean reward per episode: 122.05\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 600      |\n",
            "| mean 100 episode reward | 122      |\n",
            "| steps                   | 61695    |\n",
            "--------------------------------------\n",
            "62535 timesteps\n",
            "Best mean reward: 152.91 - Last mean reward per episode: 124.22\n",
            "63422 timesteps\n",
            "Best mean reward: 152.91 - Last mean reward per episode: 127.08\n",
            "64459 timesteps\n",
            "Best mean reward: 152.91 - Last mean reward per episode: 129.79\n",
            "65539 timesteps\n",
            "Best mean reward: 152.91 - Last mean reward per episode: 131.99\n",
            "66427 timesteps\n",
            "Best mean reward: 152.91 - Last mean reward per episode: 133.42\n",
            "67556 timesteps\n",
            "Best mean reward: 152.91 - Last mean reward per episode: 136.80\n",
            "68387 timesteps\n",
            "Best mean reward: 152.91 - Last mean reward per episode: 139.98\n",
            "69453 timesteps\n",
            "Best mean reward: 152.91 - Last mean reward per episode: 144.29\n",
            "70347 timesteps\n",
            "Best mean reward: 152.91 - Last mean reward per episode: 148.02\n",
            "71527 timesteps\n",
            "Best mean reward: 152.91 - Last mean reward per episode: 154.26\n",
            "Saving new best model\n",
            "72541 timesteps\n",
            "Best mean reward: 154.26 - Last mean reward per episode: 158.86\n",
            "Saving new best model\n",
            "73556 timesteps\n",
            "Best mean reward: 158.86 - Last mean reward per episode: 164.69\n",
            "Saving new best model\n",
            "74498 timesteps\n",
            "Best mean reward: 164.69 - Last mean reward per episode: 172.59\n",
            "Saving new best model\n",
            "75485 timesteps\n",
            "Best mean reward: 172.59 - Last mean reward per episode: 180.98\n",
            "Saving new best model\n",
            "76485 timesteps\n",
            "Best mean reward: 180.98 - Last mean reward per episode: 190.74\n",
            "Saving new best model\n",
            "77485 timesteps\n",
            "Best mean reward: 190.74 - Last mean reward per episode: 200.50\n",
            "Saving new best model\n",
            "78485 timesteps\n",
            "Best mean reward: 200.50 - Last mean reward per episode: 210.24\n",
            "Saving new best model\n",
            "79485 timesteps\n",
            "Best mean reward: 210.24 - Last mean reward per episode: 218.78\n",
            "Saving new best model\n",
            "80485 timesteps\n",
            "Best mean reward: 218.78 - Last mean reward per episode: 228.51\n",
            "Saving new best model\n",
            "81485 timesteps\n",
            "Best mean reward: 228.51 - Last mean reward per episode: 235.57\n",
            "Saving new best model\n",
            "82485 timesteps\n",
            "Best mean reward: 235.57 - Last mean reward per episode: 242.82\n",
            "Saving new best model\n",
            "83084 timesteps\n",
            "Best mean reward: 242.82 - Last mean reward per episode: 244.79\n",
            "Saving new best model\n",
            "84468 timesteps\n",
            "Best mean reward: 244.79 - Last mean reward per episode: 251.55\n",
            "Saving new best model\n",
            "85521 timesteps\n",
            "Best mean reward: 251.55 - Last mean reward per episode: 256.49\n",
            "Saving new best model\n",
            "86406 timesteps\n",
            "Best mean reward: 256.49 - Last mean reward per episode: 259.45\n",
            "Saving new best model\n",
            "87375 timesteps\n",
            "Best mean reward: 259.45 - Last mean reward per episode: 261.53\n",
            "Saving new best model\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 700      |\n",
            "| mean 100 episode reward | 260      |\n",
            "| steps                   | 87650    |\n",
            "--------------------------------------\n",
            "88399 timesteps\n",
            "Best mean reward: 261.53 - Last mean reward per episode: 246.31\n",
            "89429 timesteps\n",
            "Best mean reward: 261.53 - Last mean reward per episode: 246.05\n",
            "90550 timesteps\n",
            "Best mean reward: 261.53 - Last mean reward per episode: 235.71\n",
            "91500 timesteps\n",
            "Best mean reward: 261.53 - Last mean reward per episode: 222.65\n",
            "92327 timesteps\n",
            "Best mean reward: 261.53 - Last mean reward per episode: 222.06\n",
            "93528 timesteps\n",
            "Best mean reward: 261.53 - Last mean reward per episode: 222.41\n",
            "94528 timesteps\n",
            "Best mean reward: 261.53 - Last mean reward per episode: 227.51\n",
            "95330 timesteps\n",
            "Best mean reward: 261.53 - Last mean reward per episode: 230.47\n",
            "96439 timesteps\n",
            "Best mean reward: 261.53 - Last mean reward per episode: 219.41\n",
            "97092 timesteps\n",
            "Best mean reward: 261.53 - Last mean reward per episode: 216.07\n",
            "98564 timesteps\n",
            "Best mean reward: 261.53 - Last mean reward per episode: 215.79\n",
            "99173 timesteps\n",
            "Best mean reward: 261.53 - Last mean reward per episode: 211.88\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<stable_baselines.deepq.dqn.DQN at 0x7f4b532fd550>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "id": "KnbUFmiRLku3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def movingAverage(values, window):\n",
        "    \"\"\"\n",
        "    Smooth values by doing a moving average\n",
        "    :param values: (numpy array)\n",
        "    :param window: (int)\n",
        "    :return: (numpy array)\n",
        "    \"\"\"\n",
        "    weights = np.repeat(1.0, window) / window\n",
        "    return np.convolve(values, weights, 'valid')\n",
        "\n",
        "\n",
        "def plot_results(log_folder, title='Learning Curve For CartPole'):\n",
        "    \"\"\"\n",
        "    plot the results\n",
        "\n",
        "    :param log_folder: (str) the save location of the results to plot\n",
        "    :param title: (str) the title of the task to plot\n",
        "    \"\"\"\n",
        "    x, y = ts2xy(load_results(log_folder), 'timesteps')\n",
        "    y = movingAverage(y, window=50)\n",
        "    #Truncate x\n",
        "    x = x[len(x) - len(y):]\n",
        "\n",
        "    fig = plt.figure(title)\n",
        "    plt.plot(x, y)\n",
        "    plt.xlabel('Number of Timesteps')\n",
        "    plt.ylabel('Rewards')\n",
        "    plt.title(title + \" Smoothed\")\n",
        "    plt.title(title)\n",
        "    plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-3eqlft__ltZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plot_results(log_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}